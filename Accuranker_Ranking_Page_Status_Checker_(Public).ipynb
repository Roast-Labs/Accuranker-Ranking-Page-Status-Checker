{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# This script is brought to by ROAST Labs the internal development and innovation team at [ROAST](https://weareroast.com/) a digital marketing agency.\n",
        "\n",
        "**Purpose:**\n",
        "\n",
        "This Python script interfaces with the AccuRanker API to fetch domain information and keywords associated with a selected domain. It checks the HTTP status of URLs linked with each keyword and logs this data into a CSV file along with other keyword details.\n",
        "\n",
        "**How to use**\n",
        "\n",
        "1.   Add in your [Accuranker API key on line 5](https://app.accuranker.com/app/api?_gl=1*2h0ngs*_ga*MjAxODIxNjAzOS4xNzE1MDkzNjgx*_ga_D2SXNT78V1*MTcxNTA5MzY4MC4xLjEuMTcxNTA5MzY4Mi41OC4wLjE5NjI4MzIyMjE.#section/Introduction).\n",
        "2.   Run the code\n",
        "3.   Select a domain by type a number from the domains listed\n",
        "4.   Script will say how many key phrases are be checked, type yes to continue\n",
        "5.   The script will run and then save a csv in the sample_data folder.\n",
        "\n",
        "\n",
        "PLease note you run this script at your own risk, ROAST has no liability for the running of this or any other scripts."
      ],
      "metadata": {
        "id": "Ld1O4Lt6_LFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "# Constants\n",
        "ACCURANKER_API_TOKEN = 'insert-you-api-key-here'\n",
        "HEADERS = {\n",
        "    'Authorization': f'Token {ACCURANKER_API_TOKEN}',\n",
        "    'Content-Type': 'application/json'\n",
        "}\n",
        "\n",
        "def get_domains():\n",
        "    url = 'https://app.accuranker.com/api/v4/domains/?fields=domain,display_name,id'\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "    response.raise_for_status()\n",
        "    return response.json()\n",
        "\n",
        "def select_domain(domains):\n",
        "    for idx, domain in enumerate(domains, 1):\n",
        "        print(f\"{idx}. {domain['display_name']} ({domain['domain']})\")\n",
        "    selection = int(input(\"Select a domain by number: \"))\n",
        "    return domains[selection - 1]['id']\n",
        "\n",
        "def get_keywords(domain_id):\n",
        "    url = f'https://app.accuranker.com/api/v4/domains/{domain_id}/keywords/?fields=id,keyword,search_type,ranks.id,ranks.rank,ranks.page_serp_features.ads_top,ranks.page_serp_features.ads_bottom,ranks.highest_ranking_page,ranks.created_at,ranks.rank,ranks.extra_ranks'\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "    response.raise_for_status()\n",
        "    return response.json()  # This returns a list directly\n",
        "\n",
        "def check_http_status(url):\n",
        "    if url is None:\n",
        "        return \"no check\"\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)  # GET request with timeout\n",
        "        return response.status_code\n",
        "    except requests.RequestException as e:\n",
        "        return str(e)\n",
        "\n",
        "def process_keyword_data(keywords):\n",
        "    print(f\"Total keywords to be checked: {len(keywords)}\")\n",
        "    confirm = input(\"Do you want to proceed with URL checks? (yes/no): \")\n",
        "    if confirm.lower() != 'yes':\n",
        "        return []\n",
        "\n",
        "    processed_data = []\n",
        "    status_counts = {}\n",
        "    total_checked = 0\n",
        "\n",
        "    for i, keyword in enumerate(keywords, 1):\n",
        "        for rank in keyword['ranks']:\n",
        "            total_checked += 1\n",
        "            highest_ranking_page_status = check_http_status(rank['highest_ranking_page'])\n",
        "            status_counts[highest_ranking_page_status] = status_counts.get(highest_ranking_page_status, 0) + 1\n",
        "            row_data = {\n",
        "                'id': keyword['id'],\n",
        "                'keyword': keyword['keyword'],\n",
        "                'search_type': keyword['search_type'],\n",
        "                'rank_id': rank['id'],\n",
        "                'created_at': rank['created_at'],\n",
        "                'rank': rank['rank'],\n",
        "                'highest_ranking_page': rank['highest_ranking_page'],\n",
        "                'highest_ranking_page_status': highest_ranking_page_status,\n",
        "                'ads_top': rank['page_serp_features']['ads_top'],\n",
        "                'ads_bottom': rank['page_serp_features']['ads_bottom'],\n",
        "            }\n",
        "            processed_data.append(row_data)\n",
        "            print(f\"Checked {total_checked}/{len(keywords)} keywords...\")\n",
        "    print(\"URL check complete.\")\n",
        "    print(f\"Total URLs checked: {total_checked}\")\n",
        "    for status, count in status_counts.items():\n",
        "        print(f\"HTTP {status}: {count} times\")\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "def write_to_csv(data):\n",
        "    date_time = datetime.now().strftime(\"%y%m%d %H %M\")\n",
        "    filename = f\"/content/sample_data/Accuranker Ranking Page Status Check {date_time}.csv\"\n",
        "\n",
        "    with open(filename, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "\n",
        "        # Write the header\n",
        "        header = [\n",
        "            'id', 'keyword', 'search_type', 'rank_id', 'created_at', 'rank',\n",
        "            'highest_ranking_page', 'highest_ranking_page_status', 'ads_top', 'ads_bottom'\n",
        "        ]\n",
        "        writer.writerow(header)\n",
        "\n",
        "        # Write the rows\n",
        "        for keyword_data in data:\n",
        "            row = [\n",
        "                keyword_data['id'], keyword_data['keyword'], keyword_data['search_type'],\n",
        "                keyword_data['rank_id'], keyword_data['created_at'], keyword_data['rank'],\n",
        "                keyword_data['highest_ranking_page'], keyword_data['highest_ranking_page_status'],\n",
        "                keyword_data['ads_top'], keyword_data['ads_bottom']\n",
        "            ]\n",
        "            writer.writerow(row)\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "# Main execution\n",
        "domains = get_domains()\n",
        "domain_id = select_domain(domains)\n",
        "keywords = get_keywords(domain_id)\n",
        "processed_data = process_keyword_data(keywords)\n",
        "write_to_csv(processed_data)\n"
      ],
      "metadata": {
        "id": "yKyY6c_nqzVN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
